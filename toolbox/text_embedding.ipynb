{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An embedding is a vector (list) of floating point numbers. The distance between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "\n",
    "Embeddings are commonly used for:\n",
    "\n",
    "* Search (where results are ranked by relevance to a query string)\n",
    "* Clustering (where text strings are grouped by similarity)\n",
    "* Recommendations (where items with related text strings are recommended)\n",
    "* Anomaly detection (where outliers with little relatedness are identified)\n",
    "* Diversity measurement (where similarity distributions are analyzed)\n",
    "* Classification (where text strings are classified by their most similar label)\n",
    "\n",
    "The common process of text analysis is:\n",
    "* Tokenization\n",
    "* Embedding\n",
    "* Similarity/distance calculation\n",
    "\n",
    "\n",
    "Reference: https://platform.openai.com/docs/guides/embeddings/what-are-embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenAI embedding model\n",
    "\t\t\n",
    "* MODEL: text-embedding-ada-002\t\n",
    "* TOKENIZER: cl100k_base\t\n",
    "* MAX INPUT TOKENS: 8191\n",
    "* OUTPUT DIMENSIONS: 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-ada-002\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return openai.Embedding.create(input = [text], model=model)['data'][0]['embedding']\n",
    "\n",
    "\n",
    "#df.text is the column with the text\n",
    "\n",
    "#1. Get the embedding for each text\n",
    "df['ada_embedding'] = df.text.apply(lambda x: get_embedding(x, model='text-embedding-ada-002'))\n",
    "\n",
    "#2. Save them for future reuse\n",
    "df.to_csv('output/embedded_text.csv', index=False)\n",
    "\n",
    "#3. Load them\n",
    "df = pd.read_csv('output/embedded_text.csv')\n",
    "df['ada_embedding'] = df.ada_embedding.apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#View the number of tokens\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "# Load the cl100k_base tokenizer which is designed to work with the ada-002 model\n",
    "tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# Tokenize the text and save the number of tokens to a new column\n",
    "df['n_tokens'] = df.text.apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "# Visualize the distribution of the number of tokens per row using a histogram\n",
    "df.n_tokens.hist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KeyBERT: a minimal and easy-to-use keyword extraction technique that leverages BERT embeddings to create keywords and keyphrases that are most similar to a document.\n",
    "\n",
    "* Ref: https://maartengr.github.io/KeyBERT/\n",
    "* Use cases: apps/central_bank_speech_BERT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
